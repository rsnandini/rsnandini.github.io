---
layout: post
title: Coresets for Data-efficient Deep Learning
date: 2023-06-11 15:53:00-0400
description: Towards Sustainable Learning
categories: data-efficiency
giscus_comments: true
related_posts: true
---


To improve the efficiency and sustainability of learning deep models, we propose <b>CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models</b>, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI, confirm that CREST speeds up training deep networks on very large datasets, by 1.7x to 2.5x with minimum loss in the performance. By analyzing the learning difficulty of the subsets selected by CREST, we show that deep models benefit the most by learning from subsets of increasing difficulty levels.

Read the full article here: [paper](../../../assets/pdf/yang23sustainable.pdf)


<h3>Background: Coresets for Efficient Training</h3>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        When training machine learning models, not all examples are equally important or necessary for achieving good performance. By selecting the most relevant data for training, we can improve the efficiency of the training process.<br /><br />
        <a href="../../../assets/pdf/mirzasoleiman20data.pdf">Recent research</a> has shown that for strongly convex models (linear and ridge regression, and regularized support vector machines), one <b>weighted subset (coreset)</b> is enough to (1) upper bound the gradient error during the entire training, and (2) guarantee convergence for (Incremental) Gradient Descent.
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/efficient.png" title="efficient" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<br>

<h3>Extracting Coresets for Deep Neural Networks is Challenging!</h3>
Because (mini-batch) SGD needs mini-batches with small bias and variance to converge to a stationary point, there are two challenges of extracting coresets for deep models:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
            <h5><b>Non-convex Loss</b> &rarr; <b>High Bias</b></h5>
            Loss and gradient change rapidly because of the non-convexity of the loss function â†’ One subset cannot upper bound the gradient during the entire training
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <h5><b>Stochastic Gradient</b> &rarr; <b>High Variance</b></h5>
        One weighted subset selected from the full data has large variance when trained on with mini-batches
    </div>
</div>

<br>

<h3>Introducing CREST: <ins>C</ins>o<ins>re</ins>sets for <ins>St</ins>ochastic Gradient Descent</h3>
<h4>Modeling the non-convex loss as a <span class="emp">piece-wise quadratic</span></h4>
1. Select one coreset at $w$
2. Model a quadratic function with Taylor expansion of $$\mathcal{L}(w)$$
3. Train on the selected subset as long as the quadratic function has a small approximation error

<h4>Selecting <s>a single coreset</s> <span class="emp">mini-batch coresets</span> from <span class="emp">random subsets</span> of all data</h4>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        Instead of one coreset, we want to find multiple mini-batch coresets from larger random subsets to have smaller variance.
        Each of the mini-batch coresets is nearly unbiased as long as the quadratic approximation is still valid.
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/multiple.gif" title="multiple" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<h3> Theoritical Results </h3>
<b>Theorem (Informal).</b> Assuming a bounded variance $$\sigma^2$$ on the stochastic gradient, if the gradient bias of mini-batch coresets is bounded by the gradient norm

1. the selected coresets have small gradient error at beginning of region 
2. small $$\tau$$ ensures the error remains small during the region 
training with stochastic gradient descent on mini-batch coresets found by CREST from random subset of size $$r$$ converges to a stationary point in the following number of iterations:
<div class="row justify-content-sm-center">
    $$\mathcal{O}(\frac{L(\mathcal{L}(w_0)-\mathcal{L}^*)}{\nu^2}(1+\frac{\sigma^2}{r\nu^2}))$$
</div>
<br />

<h3>Emprical Results</h3>
<h4>CREST outperforms state-of-the-art coreset selection baselines</h4>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/table.png" title="table" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<br />

<h4>CREST speedups training on image and language data</h4>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/speedup.png" title="speedup" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<br />

<h3>CREST reveals what data is good for deep models</h3>

<h4>Deep models needs data of increasing difficulty</h4>

<div class="row justify-content-sm-center">
    {% include figure.html path="assets/img/blog/forgetting.png" title="forgetting" class="img-fluid rounded z-depth-1" %}
</div>

<h4> BibTeX </h4>

```html
@inproceedings{yang2023towards,
      title={Towards Sustainable Learning: Coresets for Data-efficient Deep Learning},
      author={Yang, Yu and Kang, Hao and Mirzasoleiman, Baharan},
      booktitle={Proceedings of the 40th International Conference on Machine Learning},
      year={2023}
    }
```     

<h4> Code </h4>
<div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center">
  {% include repository/repo.html repository='BigML-CS-UCLA/Crest' %}
</div>

